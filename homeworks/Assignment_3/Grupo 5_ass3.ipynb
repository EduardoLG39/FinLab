{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a367ce-7e68-4034-a3a2-2425e86ea7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa5d87-0eed-4cc5-8c42-68360830bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_D = \"2022-01-01\"\n",
    "END_D   = \"2025-12-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e5441-4eb7-42f1-9105-f306877e5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4f601-2a88-4d00-a6ad-9179cfdc0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#CONFIGURATION\n",
    "CACHE_DIR = Path(\".cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "#CACHE UTILITIES (PARQUET -> csv fallback)\n",
    "\n",
    "def _hash_key(*parts: object) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    for p in parts:\n",
    "        h.update(str(p).encode(\"utf-8\"))\n",
    "        h.update(b\"|\")\n",
    "    return h.hexdigest()[:24]\n",
    "def _cache_paths(prefix: str, key: str) -> tuple[Path, Path]:\n",
    "    p_parquet = CACHE_DIR / f\"{prefix}_{key}.parquet\"\n",
    "    p_csv = CACHE_DIR / f\"{prefix}_{key}.csv\"\n",
    "    return p_parquet, p_csv\n",
    "\n",
    "def _read_cache_df(prefix: str, key: str) -> pd.DataFrame | None:\n",
    "    p_parquet, p_csv = _cache_paths(prefix, key)\n",
    "    if p_parquet.exists():\n",
    "        try:\n",
    "            return pd.read_parquet(p_parquet)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if p_csv.exists():\n",
    "        try:\n",
    "            return pd.read_csv(p_csv)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _write_cache_df(df: pd.DataFrame, prefix: str, key: str) -> None:\n",
    "    p_parquet, p_csv = _cache_paths(prefix, key)\n",
    "    try:\n",
    "        df.to_parquet(p_parquet, index=False)\n",
    "        return\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        df.to_csv(p_csv, index=False)\n",
    "    except Exception:\n",
    "        # If caching fails, ignore silently (no exceptions)\n",
    "        return\n",
    "\n",
    "# HTTP helpers\n",
    "\n",
    "def http_get_text(url: str, timeout: int = 30, headers: dict | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Returns response text. Raises inside, but callers wrap in try/except (no exceptions to user).\n",
    "    \"\"\"\n",
    "    import requests\n",
    "\n",
    "    h = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; FinanceCourse/1.0)\",\n",
    "        \"Accept\": \"*/*\",\n",
    "    }\n",
    "    if headers:\n",
    "        h.update(headers)\n",
    "\n",
    "    r = requests.get(url, timeout=timeout, headers=h)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "# Numeric + returns helpers\n",
    "def parse_number(x) -> float:\n",
    "    \"\"\"\n",
    "    Robust number parser:\n",
    "      - '3,367' -> 3.367 (comma decimal)\n",
    "      - '1,234.56' -> 1234.56 (comma thousands)\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return np.nan\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"nan\", \"none\"}:\n",
    "        return np.nan\n",
    "\n",
    "    if \",\" in s and \".\" in s:\n",
    "        s = s.replace(\",\", \"\")\n",
    "    elif \",\" in s and \".\" not in s:\n",
    "        s = s.replace(\",\", \".\")\n",
    "\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def pct_change(s: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return x.pct_change(fill_method=None).replace([np.inf, -np.inf], np.nan)\n",
    "# BCRP parsing\n",
    "\n",
    "_SP2EN = {\n",
    "    \"Ene\": \"Jan\", \"Feb\": \"Feb\", \"Mar\": \"Mar\", \"Abr\": \"Apr\", \"May\": \"May\", \"Jun\": \"Jun\",\n",
    "    \"Jul\": \"Jul\", \"Ago\": \"Aug\", \"Set\": \"Sep\", \"Sep\": \"Sep\", \"Oct\": \"Oct\", \"Nov\": \"Nov\", \"Dic\": \"Dec\",\n",
    "}\n",
    "\n",
    "def _clean_bcrp_payload(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    BCRP sometimes returns \"CSV\" wrapped as HTML with <br> line breaks.\n",
    "    Normalize to plain text with real newlines.\n",
    "    \"\"\"\n",
    "    x = txt.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    x = x.replace(\"<br/>\", \"\\n\").replace(\"<br />\", \"\\n\").replace(\"<br>\", \"\\n\")\n",
    "    x = re.sub(r\"</?pre[^>]*>\", \"\", x, flags=re.IGNORECASE)\n",
    "    return x.strip()\n",
    "\n",
    "def _detect_sep(header_line: str) -> str:\n",
    "    return \";\" if header_line.count(\";\") > header_line.count(\",\") else \",\"\n",
    "\n",
    "def _parse_bcrp_date(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parses common BCRP date formats:\n",
    "      - Daily: 2022-01-03, 03Jan22, 03Ene22\n",
    "      - Monthly: Jan22, Ene22, 2022-1, 2022-01\n",
    "      - Yearly: 2022\n",
    "    \"\"\"\n",
    "    x = s.astype(str).str.strip()\n",
    "    x = x.str.replace(\".\", \"\", regex=False)\n",
    "\n",
    "    y = x\n",
    "    for k, v in _SP2EN.items():\n",
    "        y = y.str.replace(k, v, regex=False)\n",
    "\n",
    "    dt = pd.to_datetime(y, format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "    m = dt.isna()\n",
    "    if m.any():\n",
    "        dt.loc[m] = pd.to_datetime(y[m], format=\"%d%b%y\", errors=\"coerce\")\n",
    "\n",
    "    m = dt.isna()\n",
    "    if m.any():\n",
    "        dt.loc[m] = pd.to_datetime(y[m], format=\"%b%y\", errors=\"coerce\")\n",
    "\n",
    "    m = dt.isna()\n",
    "    if m.any():\n",
    "        dt.loc[m] = pd.to_datetime(y[m], format=\"%Y-%m\", errors=\"coerce\")\n",
    "\n",
    "    m = dt.isna()\n",
    "    if m.any():\n",
    "        dt.loc[m] = pd.to_datetime(y[m], format=\"%Y\", errors=\"coerce\")\n",
    "\n",
    "    m = dt.isna()\n",
    "    if m.any():\n",
    "        dt.loc[m] = pd.to_datetime(y[m], errors=\"coerce\")\n",
    "\n",
    "    return dt\n",
    "\n",
    "\n",
    "def bcrp_series_csv(\n",
    "    series_codes: list[str],\n",
    "    start: str,\n",
    "    end: str,\n",
    "    lang: str = \"ing\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    BCRPData API (CSV):\n",
    "      https://estadisticas.bcrp.gob.pe/estadisticas/series/api/[codes]/csv/[start]/[end]/[lang]\n",
    "\n",
    "    Returns LONG DataFrame:\n",
    "      date, series_name, value\n",
    "\n",
    "    If the endpoint fails, prints a short message and returns an empty DataFrame (no exceptions).\n",
    "    \"\"\"\n",
    "    codes = \"-\".join(series_codes)\n",
    "    url = f\"https://estadisticas.bcrp.gob.pe/estadisticas/series/api/{codes}/csv/{start}/{end}/{lang}\"\n",
    "\n",
    "    key = _hash_key(\"bcrp\", url)\n",
    "    cached = _read_cache_df(\"bcrp\", key)\n",
    "    if cached is not None and cached.shape[0] > 0:\n",
    "        cached[\"date\"] = pd.to_datetime(cached[\"date\"], errors=\"coerce\")\n",
    "        return cached\n",
    "\n",
    "    try:\n",
    "        txt = http_get_text(url, timeout=30)\n",
    "        txt = _clean_bcrp_payload(txt)\n",
    "\n",
    "        lines = [ln for ln in txt.split(\"\\n\") if ln.strip() != \"\"]\n",
    "        if len(lines) < 2:\n",
    "            print(\"[BCRP] Endpoint returned no usable rows. Continuing...\")\n",
    "            return pd.DataFrame(columns=[\"date\", \"series_name\", \"value\"])\n",
    "\n",
    "        sep = _detect_sep(lines[0])\n",
    "        df = pd.read_csv(StringIO(\"\\n\".join(lines)), sep=sep)\n",
    "\n",
    "        if df.shape[0] == 0 or df.shape[1] < 2:\n",
    "            print(\"[BCRP] Returned an empty table. Continuing...\")\n",
    "            return pd.DataFrame(columns=[\"date\", \"series_name\", \"value\"])\n",
    "\n",
    "        date_col = df.columns[0]\n",
    "        value_cols = list(df.columns[1:])\n",
    "\n",
    "        out = df.melt(\n",
    "            id_vars=[date_col],\n",
    "            value_vars=value_cols,\n",
    "            var_name=\"series_name\",\n",
    "            value_name=\"value_raw\",\n",
    "        ).rename(columns={date_col: \"date\"})\n",
    "\n",
    "        out[\"date\"] = _parse_bcrp_date(out[\"date\"])\n",
    "        out[\"value\"] = out[\"value_raw\"].map(parse_number)\n",
    "\n",
    "        out = out.drop(columns=[\"value_raw\"])\n",
    "        out = out.dropna(subset=[\"date\"]).sort_values([\"series_name\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "        _write_cache_df(out, \"bcrp\", key)\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[BCRP] Endpoint unavailable ({type(e).__name__}). Continuing...\")\n",
    "        return pd.DataFrame(columns=[\"date\", \"series_name\", \"value\"])\n",
    "# Yahoo Finance (yfinance)\n",
    "\n",
    "def yfinance_download(tickers: list[str], start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Yahoo Finance via yfinance.\n",
    "    Returns LONG DataFrame:\n",
    "      date, ticker, close, volume, ret\n",
    "\n",
    "    If the endpoint fails, prints a short message and returns an empty DataFrame (no exceptions).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import yfinance as yf\n",
    "    except Exception:\n",
    "        print(\"[yfinance] yfinance not installed/importable. Continuing...\")\n",
    "        return pd.DataFrame(columns=[\"date\", \"ticker\", \"close\", \"volume\", \"ret\"])\n",
    "\n",
    "    key = _hash_key(\"yfinance\", \" \".join(tickers), start, end)\n",
    "    cached = _read_cache_df(\"yf\", key)\n",
    "    if cached is not None and cached.shape[0] > 0:\n",
    "        cached[\"date\"] = pd.to_datetime(cached[\"date\"], errors=\"coerce\")\n",
    "        return cached\n",
    "\n",
    "    try:\n",
    "        data = yf.download(tickers=tickers, start=start, end=end, auto_adjust=False, progress=False)\n",
    "        if data is None or data.shape[0] == 0:\n",
    "            print(\"[yfinance] Returned no rows. Continuing...\")\n",
    "            return pd.DataFrame(columns=[\"date\", \"ticker\", \"close\", \"volume\", \"ret\"])\n",
    "\n",
    "        frames = []\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            for t in tickers:\n",
    "                if t not in data.columns.get_level_values(1):\n",
    "                    continue\n",
    "                sub = data.xs(t, axis=1, level=1).copy()\n",
    "                sub = sub.reset_index().rename(columns={\"Date\": \"date\", \"Datetime\": \"date\"})\n",
    "                sub[\"ticker\"] = t\n",
    "                sub = sub.rename(columns={\"Close\": \"close\", \"Volume\": \"volume\"})\n",
    "                frames.append(sub[[\"date\", \"ticker\", \"close\", \"volume\"]])\n",
    "            out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "        else:\n",
    "            out = data.reset_index().rename(columns={\"Date\": \"date\", \"Datetime\": \"date\"})\n",
    "            out[\"ticker\"] = tickers[0]\n",
    "            out = out.rename(columns={\"Close\": \"close\", \"Volume\": \"volume\"})\n",
    "            out = out[[\"date\", \"ticker\", \"close\", \"volume\"]]\n",
    "\n",
    "        out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "        out[\"close\"] = pd.to_numeric(out[\"close\"], errors=\"coerce\")\n",
    "        out[\"volume\"] = pd.to_numeric(out[\"volume\"], errors=\"coerce\")\n",
    "\n",
    "        out = out.dropna(subset=[\"date\", \"close\"]).sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "        out[\"ret\"] = out.groupby(\"ticker\")[\"close\"].apply(pct_change).reset_index(level=0, drop=True)\n",
    "\n",
    "        _write_cache_df(out, \"yf\", key)\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[yfinance] Endpoint unavailable ({type(e).__name__}). Continuing...\")\n",
    "        return pd.DataFrame(columns=[\"date\", \"ticker\", \"close\", \"volume\", \"ret\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc99306-5619-4b9e-ac7a-2d0790a331a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_D = \"2022-01-01\"\n",
    "END_D   = \"2025-12-18\"\n",
    "\n",
    "peru = bcrp_series_csv(\n",
    "    series_codes=[\"PD04637PD\", \"PD04639PD\", \"PD04704XD\", \"PD04701XD\"],\n",
    "    start=START_D,\n",
    "    end=END_D,\n",
    "    lang=\"ing\"\n",
    ")\n",
    "\n",
    "usa = yfinance_download([\"SPY\", \"TLT\", \"GLD\"], start=START_D, end=END_D)\n",
    "\n",
    "print(\"Peru rows:\", peru.shape[0], \"| USA rows:\", usa.shape[0])\n",
    "\n",
    "# show in English (wide + renamed)\n",
    "peru_wide = (\n",
    "    peru.pivot_table(index=\"date\", columns=\"series_name\", values=\"value\", aggfunc=\"last\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"date\")\n",
    ")\n",
    "peru_wide.columns.name = None\n",
    "\n",
    "rename_map = {}\n",
    "for c in peru_wide.columns:\n",
    "    cl = str(c).lower()\n",
    "    if \"interbanc\" in cl and (\"tipo de cambio\" in cl or \"tc\" in cl or \"exchange\" in cl):\n",
    "        rename_map[c] = \"fx_interbank_buy\"\n",
    "    elif \"sbs\" in cl and (\"tipo de cambio\" in cl or \"tc\" in cl or \"exchange\" in cl):\n",
    "        rename_map[c] = \"fx_sbs_buy\"\n",
    "    elif \"oro\" in cl or \"gold\" in cl:\n",
    "        rename_map[c] = \"gold_london\"\n",
    "    elif \"cobre\" in cl or \"copper\" in cl:\n",
    "        rename_map[c] = \"copper_london\"\n",
    "\n",
    "peru_wide = peru_wide.rename(columns=rename_map)\n",
    "peru_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef81b3b-9272-4698-89d5-e3489eb6da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa[\"ticker\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8e1f7-71dd-425b-b290-d9ac308086af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Distributions\n",
    "# Exercise 3.1 - Return distributions: Peru Gold vs US Gold ETF\n",
    "# El objetivo es comparar cómo se distribuyen los retornos diarios de oro de referencia del BCRP (Perú) y ETF GLD (USA)\n",
    "#1. PARTIMOS DE peru_wide\n",
    "#ORO PERÚ\n",
    "peru_gold = (\n",
    "    peru_wide[[\"date\", \"gold_london\"]]\n",
    "    .dropna()\n",
    "    .sort_values(\"date\")\n",
    ")\n",
    "#HALLAMOS LOS RETORNOS DIARIOS\n",
    "#pct_change() nos dará el retorno porcentual diario\n",
    "peru_gold[\"ret\"] = peru_gold[\"gold_london\"].pct_change()\n",
    "\n",
    "peru_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e5e4b-f206-4ad4-ad78-37ce4be65778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. From usa, filter to GLD returns\n",
    "#FILTRAMOS LOS RETORNOS DE GLD USA\n",
    "gld = (\n",
    "    usa[usa[\"ticker\"] == \"GLD\"]\n",
    "    .copy()\n",
    "    .dropna(subset=[\"ret\"])\n",
    ")\n",
    "\n",
    "gld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb5466-0c41-4d15-a27b-a81658f44f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Plot two histograms (separate figures) with the same binning.\n",
    "#PRIMERO DEFINIMOS LOS MISMOS BINS PARA QUE LA COMPARACIÓN SEA JUSTA\n",
    "all_rets = pd.concat([\n",
    "    peru_gold[\"ret\"].dropna(),\n",
    "    gld[\"ret\"].dropna()\n",
    "])\n",
    "\n",
    "bins = np.histogram_bin_edges(all_rets, bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb7452-149d-4f12-b156-42d641aad7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HISTOGRAMA 1 - ORO PERÚ\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(\n",
    "    peru_gold[\"ret\"].dropna(),\n",
    "    bins=bins,\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "plt.title(\"Distribución de Retornos — Oro Perú (BCRP)\")\n",
    "plt.xlabel(\"Retorno diario\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb6e63-27d4-4ee6-b1ba-fb7a30c12b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_gld = yfinance_download(\n",
    "    [\"GLD\"],\n",
    "    start=START_D,\n",
    "    end=END_D\n",
    ")\n",
    "\n",
    "usa_gld[\"ticker\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ade9e-b724-43ad-ac2e-c6d3d3a3d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "gld = (\n",
    "    usa_fixed[usa_fixed[\"ticker\"] == \"GLD\"]\n",
    "    .dropna(subset=[\"ret\"])\n",
    "    .sort_values(\"date\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "gld[\"ret\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e6452-899b-4973-83d4-dbc007470c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HISTOGRAMA 2 - GLD USA\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(\n",
    "    gld[\"ret\"].dropna(),\n",
    "    bins=bins,\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "plt.title(\"Distribución de Retornos — GLD (USA)\")\n",
    "plt.xlabel(\"Retorno diario\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09489b-fbfc-43f6-8482-8abbc0afe2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Optional: overlay KDE for each distribution.\n",
    "#KDE + HISTOGRAMA ORO PERÚ\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "sns.histplot(\n",
    "    peru_gold[\"ret\"].dropna(),\n",
    "    bins=bins,\n",
    "    stat=\"count\",\n",
    "    kde=True\n",
    ")\n",
    "\n",
    "plt.title(\"Distribución de Retornos — Oro Perú (BCRP)\")\n",
    "plt.xlabel(\"Retorno diario\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62157637-3ec6-41ce-bff1-facbcc1a5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KDE + HISTOGRAMA GLD USA\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "sns.histplot(\n",
    "    gld[\"ret\"].dropna(),\n",
    "    bins=bins,\n",
    "    stat=\"count\",\n",
    "    kde=True\n",
    ")\n",
    "\n",
    "plt.title(\"Distribución de Retornos — GLD (USA)\")\n",
    "plt.xlabel(\"Retorno diario\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e01c4-bf08-4a99-8a59-477747728f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3.2 — Annotations (mark a key event on a series)\n",
    "#ENCONTRAR EL DÍA CON EL MAYOR RETORNO ABSOLUTO\n",
    "#FILTRAMOS SPY DESDE usa\n",
    "spy = usa[usa[\"ticker\"] == \"SPY\"].copy()\n",
    "spy[[\"date\", \"ret\"]].dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cbace-8230-41e3-a5e3-3251bbd92824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HALLAMOS LA FECHA Y EL RETORNO \n",
    "idx_extreme = spy[\"ret\"].abs().idxmax()\n",
    "\n",
    "extreme_row = spy.loc[idx_extreme]\n",
    "\n",
    "extreme_date = extreme_row[\"date\"]\n",
    "extreme_ret  = extreme_row[\"ret\"]\n",
    "\n",
    "extreme_date, extreme_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11d195-b45a-4027-ad27-c7083c7f0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(spy[\"date\"], spy[\"ret\"])\n",
    "plt.title(\"Retornos diarios del SPY\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Retorno diario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f688b23-29db-46c6-aaba-2e1e0977d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOS ASEGURAMOS DE TENER EL SPY LIMPIO\n",
    "spy_clean = (\n",
    "    usa[usa[\"ticker\"] == \"SPY\"]\n",
    "    .dropna(subset=[\"ret\"])\n",
    "    .copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a66e6-4a7b-42f9-8fdc-e69b94a4d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCONTRAMOS EL PUNTO EXTREMO\n",
    "idx = spy_clean[\"ret\"].abs().idxmax()\n",
    "extreme_date = spy_clean.loc[idx, \"date\"]\n",
    "extreme_ret  = spy_clean.loc[idx, \"ret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e0b8c-95dc-4bd9-be62-4fdf3b41a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRÁFICO Y ANOTACIÓN\n",
    "plt.plot(spy_clean[\"date\"], spy_clean[\"ret\"], label=\"SPY retornos\")\n",
    "plt.annotate(\n",
    "    f\"Máx |ret|\\n{extreme_date.date()}\\n{extreme_ret:.2%}\",\n",
    "    xy=(extreme_date, extreme_ret),\n",
    "    xytext=(extreme_date, extreme_ret * 1.5),\n",
    "    arrowprops=dict(arrowstyle=\"->\")\n",
    ")\n",
    "\n",
    "plt.title(\"SPY — Retornos diarios con evento extremo\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Retorno\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854fc93-29ba-4a17-a063-49b602f55e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 Exercise 4.2\n",
    "#RUN A SIMPLE REGRESSION OF GLD RETURNS ON SPY RETURNS\n",
    "usa_gld = yfinance_download(\n",
    "    [\"GLD\"],\n",
    "    start=START_D,\n",
    "    end=END_D\n",
    ")\n",
    "\n",
    "usa_gld[\"ticker\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c3a15-42b6-468d-ab42-db09c5628513",
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_fixed = pd.concat([usa, usa_gld], ignore_index=True)\n",
    "usa_fixed[\"ticker\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb844e39-1182-4e32-b46d-ff29f8d2bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRIMERO CONSTRUIMOS LOS RETORNOS ALINEADOS\n",
    "#FILTRAMOS SPY Y GLD\n",
    "spy = (\n",
    "    usa_fixed[usa_fixed[\"ticker\"] == \"SPY\"]\n",
    "    [[\"date\", \"ret\"]]\n",
    "    .rename(columns={\"ret\": \"ret_SPY\"})\n",
    ")\n",
    "\n",
    "gld = (\n",
    "    usa_fixed[usa_fixed[\"ticker\"] == \"GLD\"]\n",
    "    [[\"date\", \"ret\"]]\n",
    "    .rename(columns={\"ret\": \"ret_GLD\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19215db1-5a88-4d20-b205-23935bf959d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALINEAMOS POR FECHA\n",
    "ret_aligned = (\n",
    "    pd.merge(spy, gld, on=\"date\", how=\"inner\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "ret_aligned.head()\n",
    "ret_aligned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e17e9-87fd-4480-8f63-fa489003cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULAMOS ALPHA Y BETA\n",
    "x = ret_aligned[\"ret_SPY\"]\n",
    "y = ret_aligned[\"ret_GLD\"]\n",
    "\n",
    "beta = np.cov(x, y, ddof=1)[0, 1] / np.var(x, ddof=1)\n",
    "alpha = y.mean() - beta * x.mean()\n",
    "\n",
    "alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78840b1-acb0-41c3-8506-27805d525dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCATTER + RECTA ESTIMADA\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(x, y, alpha=0.5, label=\"Datos\")\n",
    "\n",
    "x_line = np.linspace(x.min(), x.max(), 100)\n",
    "y_line = alpha + beta * x_line\n",
    "\n",
    "plt.plot(x_line, y_line, color=\"red\", label=\"Recta OLS\")\n",
    "\n",
    "plt.title(\"GLD vs SPY — Retornos diarios\")\n",
    "plt.xlabel(\"SPY return\")\n",
    "plt.ylabel(\"GLD return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07ad41-6cbe-47bf-b091-08bde6fef5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTERPRETACIÓN\n",
    "#El coeficiente β mide la sensibilidad de los retornos de GLD frente a los retornos del mercado accionario estadounidense (SPY). \n",
    "#Un valor positivo de β indica que, en promedio, cuando SPY sube, GLD también tiende a subir. Sin embargo, dado que el valor de \n",
    "#β es menor que 1, la reacción de GLD es más moderada que la del mercado accionario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
